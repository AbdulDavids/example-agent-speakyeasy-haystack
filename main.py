import os
from haystack import Pipeline
from haystack.dataclasses import ChatMessage
from haystack.components.builders import ChatPromptBuilder
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack_experimental.components.tools.openapi import OpenAPITool, LLMProvider
from dotenv import load_dotenv

load_dotenv()

# init the openapi tool wiht the F1 API schema
f1_tool = OpenAPITool(
    generator_api=LLMProvider.OPENAI,
    spec="f1_openapi_spec.yaml",

)

# prompt 
messages = [
    ChatMessage.from_system("Answer the F1 query using the API. Race names with two words should be separated by an underscore and be in lowercase."),
    ChatMessage.from_user("User asked: {{user_message}}"),
    ChatMessage.from_system("API responded: {{service_response}}")
]

builder = ChatPromptBuilder(template=messages)

# init the LLM
llm = OpenAIChatGenerator(generation_kwargs={"max_tokens": 1024})

# defining the pipeline components
pipe = Pipeline()
pipe.add_component("f1_tool", f1_tool)
pipe.add_component("builder", builder)
pipe.add_component("llm", llm)

# connecting the pipeline components
pipe.connect("f1_tool.service_response", "builder.service_response")
pipe.connect("builder.prompt", "llm.messages")

# run the pipeline with a question
def query_f1_pipeline(user_query: str):
    """
    Run the F1 bot pipeline with the user's query.
    :param user_query: The user's query as a string.
    :return: The response generated by the LLM.
    """
    result = pipe.run(data={
        "f1_tool": {
            "messages": [ChatMessage.from_user(user_query)]
        },
        "builder": {
            "user_message": ChatMessage.from_user("Answer the F1 query in a user-friendly way")
        }
    })
    return result["llm"]["replies"][0].content


if __name__ == "__main__":
    query = input("Enter your F1 query: ")  # Example: "Who won in Monaco in 2024?"
    response = query_f1_pipeline(query)
    print("LLM Response:", response)
